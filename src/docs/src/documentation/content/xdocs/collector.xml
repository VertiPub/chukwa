<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed to the Apache Software Foundation (ASF) under one or more
  contributor license agreements.  See the NOTICE file distributed with
  this work for additional information regarding copyright ownership.
  The ASF licenses this file to You under the Apache License, Version 2.0
  (the "License"); you may not use this file except in compliance with
  the License.  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE document PUBLIC "-//APACHE//DTD Documentation V2.0//EN" "http://forrest.apache.org/dtd/document-v20.dtd">

<document>
  <header>
    <title>Chukwa Collector Setup Guide</title>
  </header>
  <body>
  	<section>
  	  <title>Basic Operation</title>
  		<p>Chukwa Collectors are responsible for accepting incoming data from Agents,
  		and storing the data.
  		 Most commonly, collectors simply write all received to HDFS.  
  		In this mode, the filesystem to write to is determined by the option
  		<code>writer.hdfs.filesystem</code> in  <code>chukwa-collector-conf.xml</code>.
  		 This is the only option that you really need to specify to get a working 
  		 collector.
  		</p>
  		<p> By default, collectors listen on port 8080. This can be configured
  		in <code>chukwa-collector.conf.xml</code></p>
  	</section>
  	
  	<section><title>Configuration Knobs</title>
  	<p>There's a bunch more "standard" knobs worth knowing about. These
  	are mostly documented in <code>chukwa-collector-conf.xml</code></p>
  	
  	<p>
  	It's also possible to do limited configuration on the command line. This is
  	primarily intended for debugging.  You can say 'writer=pretend' to get the 
  	collector to print incoming chunks on standard out, or portno=xyz to override
  	the default port number.
  	</p>
  	 	<source>
  	  bin/chukwa collector writer=pretend portno=8081
  	</source>
  	</section>
  	
  	<section><title>Advanced options</title>
  	<p>
  	  There are some advanced options, not necessarily documented in the
  	collector conf file, that are helpful in using Chukwa in nonstandard ways.
  	</p> <p>
	    While normally Chukwa writes sequence files to HDFS, it's possible to
	    specify an alternate Writer class. The option 
	    <code>chukwaCollector.writerClass</code> specifies a Java class to instantiate
	    and use as a writer. See the <code>ChukwaWriter</code> javadoc for details.
	  </p>  <p>
	  	One particularly useful Writer class is <code>PipelineStageWriter</code>, which
	  	lets you string together a series of <code>PipelineableWriters</code>
	  	for pre-processing or post-processing incoming data.
	  	As an example, the SocketTeeWriter class allows other programs to get incoming chunks
	  	fed to them over a socket by the collector.
	  	</p>
	  	
	  	<p>Stages in the pipeline should be listed, comma-separated, in option 
	  	<code>chukwaCollector.pipeline</code></p>
	  	
	  	<source>
&#60;property&#62;
  &#60;name&#62;chukwaCollector.writerClass&#60;/name&#62;
  &#60;value&#62;org.apache.hadoop.chukwa.datacollection.writer.PipelineStageWriter&#60;/value&#62;
&#60;/property&#62;

&#60;property&#62;
  &#60;name&#62;chukwaCollector.pipeline&#60;/name&#62;
  &#60;value&#62;org.apache.hadoop.chukwa.datacollection.writer.SocketTeeWriter,org.apache.hadoop.chukwa.datacollection.writer.SeqFileWriter&#60;/value&#62;
&#60;/property&#62;
	  	</source>
	  	
	  	<section>
	  	<title>SocketTeeWriter</title>
	  	<p>
	  		The <code>SocketTeeWriter</code> allows external processes to watch
	  	the stream of chunks passing through the collector. This allows certain kinds
	  	of real-time monitoring to be done on-top of Chukwa.</p>  
	  	
	  	 <p>  
	  	    SocketTeeWriter listens on a port (specified by conf option
	  	 <code>chukwaCollector.tee.port</code>, defaulting to 9094.)  Applications
	  	 that want Chunks should connect to that port, and issue a command of the form
	  	 <code>RAW|WRITABLE &#60;filter&#62;\n</code>. Filters use the same syntax
	  	 as the <a href="programming.html#Reading+data+from+the+sink+or+the+archive">
	  	 Dump command</a>.  If the filter is accepted, the Writer will respond 
	  	 <code>OK\n</code>.
	  	 </p>
	  	 <p>
	  	 Subsequently, Chunks matching the filter will be serialized and sent back over the socket.
	  	Specifying "WRITABLE" will cause the chunks to be written using Hadoop's 
	  	Writable serialization framework. "RAW" will send the internal data of the
	  	Chunk, without any metadata, prefixed by its length encoded as a 32-bit int,
	  	big-endian.  "HEADER" is similar to "RAW", but with a one-line header in
	  	front of the content. Header format is <code>hostname</code> 
	  	<code>datatype</code> <code>stream name</code> <code>offset</code>, separated by spaces.
	  	</p>
	  	<p>
	  	The filter will be de-activated when the socket is closed.
	  	</p>

	  	<source>
Socket s2 = new Socket("host", SocketTeeWriter.DEFAULT_PORT);
s2.getOutputStream().write("RAW datatype=XTrace\n".getBytes());
dis = new DataInputStream(s2.getInputStream());
dis.readFully(new byte[3]); //read "OK\n"
while(true) {
   int len = dis.readInt();
   byte[] data = new byte[len];
   dis.readFully(data);
   DoSomethingUsing(data);
}
	  	</source>
	  	</section>
	  	

  	</section>
  </body>
</document>
